{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "default_args={\n",
    "        'owner':'Kranthi',\n",
    "        'start_date':datetime(2023,5,5),\n",
    "        'retries':3,\n",
    "        'retry_delay':timedelta(minutes=5)\n",
    "        }\n",
    "\n",
    "Timberland_stock_analysis= DAG('Timberland_stock_analysis',\n",
    "          default_args=default_args,\n",
    "          description='Timberland_stock_analysis',\n",
    "          schedule_interval='* * * * *',\n",
    "          catchup=False,\n",
    "          tags=['example,helloworld']\n",
    "          )        \n",
    "\n",
    "# Task 1: Dummy Operator to start the task\n",
    "task1 = DummyOperator(task_id='start task', dag=dag)\n",
    "\n",
    "# Task 2: Run Spark job to read CSV and send output file\n",
    "def run_spark_job():\n",
    "    # Read CSV file using Spark and perform required processing\n",
    "    # Save the output file to /root/airflow/outputfiles/\n",
    "\n",
    "    # Example Spark job code:\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    spark = SparkSession.builder.appName(\"AirflowProject\").getOrCreate()\n",
    "    \n",
    "    # Read CSV file\n",
    "    csv_data = spark.read.option(\"header\" , True).csv(\"/root/airflow/inputfiles/timberland_stock.csv\")\n",
    "    csv_data.createOrReplaceTempView(\"mytable\")\n",
    "    Peak_High_Price_Date = spark.sql(\"select Date from mytable where High = (select max(High) from mytable)\")\n",
    "    Mean_Of_Close_Column = spark.sql(\"select avg(Close) as mean_of_column from mytable\")\n",
    "    Max_of_Volume_Column = spark.sql(\"select max(Volume) as max_of_volume from mytable\")\n",
    "    Min_of_Volume_Column = spark.sql(\"select min(Volume) as min_of_volume from mytable\")\n",
    "    No_Of_days = spark.sql(\"SELECT COUNT(*) AS count_lower_than_60 FROM mytable WHERE Close < 60\")\n",
    "    percentage = spark.sql(\"SELECT (COUNT(CASE WHEN High > 80 THEN 1 END) / COUNT(*)) * 100 AS percentage_high_above_80 FROM mytable\")\n",
    "    Pearson_Correlation  = spark.sql(\"SELECT corr(High, Volume) AS correlation FROM mytable\")\n",
    "    Max_High_Year = spark.sql(\"\"\"SELECT YEAR(Date) AS Year, MAX(CAST(High AS DOUBLE)) AS max_high FROM mytable GROUP BY Year ORDER BY Year\"\"\")\n",
    "    Avg_Close_For_Each_Month = spark.sql(\"\"\"SELECT YEAR(Date) AS Year, MONTH(Date) AS Month, AVG(Close) AS AvgClose FROM mytable GROUP BY Year, Month ORDER BY Year, Month\"\"\")\n",
    "\n",
    "    # Save output file\n",
    "    Peak_High_Price_Date.write.csv('/root/airflow/outputfiles/Peak_High_Price_Date.csv', mode='overwrite', header=True)\n",
    "\n",
    "    Mean_Of_Close_Column.write.csv('/root/airflow/outputfiles/Mean_Of_Close_Column.csv', mode='overwrite', header=True)\n",
    "\n",
    "    Max_of_Volume_Column.write.csv('/root/airflow/outputfiles/Max_of_Volume_Column.csv', mode='overwrite', header=True)\n",
    "\n",
    "    Min_of_Volume_Column.write.csv('/root/airflow/outputfiles/Min_of_Volume_Column.csv', mode='overwrite', header=True)\n",
    "\n",
    "    No_Of_days.write.csv('/root/airflow/outputfiles/No_Of_days.csv', mode='overwrite', header=True)\n",
    "\n",
    "    percentage.write.csv('/root/airflow/outputfiles/percentage.csv', mode='overwrite', header=True)\n",
    "\n",
    "    Pearson_Correlation.write.csv('/root/airflow/outputfiles/Pearson_Correlation.csv', mode='overwrite', header=True)\n",
    "\n",
    "    Max_High_Year.write.csv('/root/airflow/outputfiles/Max_High_Year.csv', mode='overwrite', header=True)\n",
    "\n",
    "    Avg_Close_For_Each_Month.write.csv('/root/airflow/outputfiles/Avg_Close_For_Each_Month.csv', mode='overwrite', header=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "task2 = PythonOperator(\n",
    "    task_id='run spark job',\n",
    "    python_callable=run_spark_job,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Task 3: Dummy Operator to end the task\n",
    "task3 = DummyOperator(task_id='end task',dag=dag)\n",
    "\n",
    "\n",
    "\n",
    "# Define task dependencies\n",
    "task1 >> task2 >> task3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
